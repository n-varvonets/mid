
# Генераторы и сопрограммы — Продвинутые примеры

## 1. Сопрограммы (Корутины) — Асинхронная обработка данных

Представим, что у нас есть задача обрабатывать большой объем данных, например, запросы к внешнему API, и нам нужно делать это асинхронно для повышения производительности.

### Пример:

```python
import asyncio

async def fetch_data(id):
    # Симулируем асинхронный запрос (например, к внешнему API)
    print(f"Запрос данных для ID {id}")
    await asyncio.sleep(2)  # симулируем задержку, как если бы это был запрос к API
    print(f"Получены данные для ID {id}")
    return {"id": id, "data": f"data_for_{id}"}

async def process_data(ids):
    results = []
    for id in ids:
        result = await fetch_data(id)
        results.append(result)
    return results

# Основная функция для запуска асинхронной задачи
async def main():
    ids = [1, 2, 3, 4, 5]
    data = await process_data(ids)
    print("Все данные обработаны:", data)

# Запуск программы
asyncio.run(main())
```

### Описание:
- В данном примере функция `fetch_data` симулирует асинхронный запрос к API с помощью `await asyncio.sleep(2)`.
- Внутри `process_data` мы выполняем запросы последовательно, дожидаясь ответа с помощью `await`.

### Оптимизация через параллельное выполнение:

Чтобы запросы выполнялись параллельно, можно использовать `asyncio.gather`, что значительно ускорит обработку данных.

```python
async def process_data_parallel(ids):
    tasks = [fetch_data(id) for id in ids]
    results = await asyncio.gather(*tasks)  # Запуск всех задач параллельно
    return results

# Основная функция для параллельного запуска
async def main_parallel():
    ids = [1, 2, 3, 4, 5]
    data = await process_data_parallel(ids)
    print("Все данные обработаны параллельно:", data)

asyncio.run(main_parallel())
```

### Описание `asyncio.gather`:
- **`asyncio.gather(*tasks)`**: запускает несколько задач одновременно, собирая результаты после их завершения. Задачи выполняются **в одном потоке** через асинхронное переключение контекста, а не через многопоточность.
- Это позволяет эффективно управлять задачами ввода-вывода (например, запросами к API) без блокировки основного потока. Задачи выполняются параллельно, так как когда одна задача "ждет", другая может выполняться.
- Асинхронная обработка используется для выполнения нескольких задач одновременно, не блокируя процесс. Это отличается от потоков, где код выполняется параллельно на уровне системы.

## 2. Генераторы — Обработка больших данных по частям

Генераторы удобны, когда нужно обрабатывать большие объемы данных по частям, не загружая в память все данные сразу. Рассмотрим пример обработки логов по частям.

### Пример:

```python
import time

def log_generator():
    logs = [
        "Log 1: user1 login",
        "Log 2: user2 upload file",
        "Log 3: user3 download file",
        "Log 4: user4 logout",
        "Log 5: user5 login"
    ]
    for log in logs:
        time.sleep(1)  # Симуляция задержки чтения больших данных
        yield log

def process_logs():
    log_gen = log_generator()
    for log in log_gen:
        print(f"Обработка: {log}")

# Запуск процесса
process_logs()
```

### Описание:
- Генератор `log_generator` возвращает логи по одному с задержкой (симуляция чтения большого файла).
- В `process_logs` мы обрабатываем логи по частям, что позволяет не загружать все данные в память сразу.

### Оптимизация через асинхронные генераторы:

Асинхронные генераторы позволяют сочетать возможности генераторов и асинхронности, что делает их удобными для работы с большими объемами данных или асинхронными источниками.

```python
import asyncio

async def async_log_generator():
    logs = [
        "Log 1: user1 login",
        "Log 2: user2 upload file",
        "Log 3: user3 download file",
        "Log 4: user4 logout",
        "Log 5: user5 login"
    ]
    for log in logs:
        await asyncio.sleep(1)  # Асинхронная задержка (симуляция обработки)
        yield log

async def process_logs_async():
    async for log in async_log_generator():
        print(f"Обработка: {log}")

# Запуск асинхронной обработки
asyncio.run(process_logs_async())
```

### Описание:
- Асинхронный генератор `async_log_generator` возвращает логи по одному с задержкой, используя `await asyncio.sleep`.
- Асинхронная обработка позволяет обрабатывать данные параллельно с другими задачами, не блокируя основной поток.

## 3. Генераторы с `send()`

Генераторы, которые принимают данные через метод `send()`, могут вести себя как корутины. Это **синхронные** корутины на базе генераторов, которые могут принимать данные во время выполнения.

### Пример:

```python
def simple_coroutine():
    print("Корутину запустили")
    value = yield
    print(f"Получено значение: {value}")
    yield

# Запуск генератора
gen = simple_coroutine()
next(gen)  # Запускаем генератор до первого yield
gen.send(42)  # Передаем значение 42 в генератор через send()
```

### Описание:
- Первый вызов `next()` запускает генератор до первого `yield`.
- Вызов `send(42)` передает значение в генератор и возобновляет его выполнение с того места, где оно было приостановлено.
- Такие генераторы иногда называют корутинами, но они **синхронные** и работают в рамках синхронного кода.

### Отличие от асинхронных корутин:
- **Генераторы с `send()`** позволяют приостанавливать выполнение и отправлять данные обратно в генератор, но они остаются синхронными.
- **Асинхронные корутины** (`async def`) работают с асинхронным кодом и позволяют управлять выполнением задач без блокировки, используя `await`.

## Заключение:
- **Сопрограммы** помогают выполнять асинхронные задачи, такие как запросы к API или работа с задержками.
- **Генераторы** полезны для поэтапной обработки больших данных, а **асинхронные генераторы** комбинируют обе техники, что делает их мощным инструментом для работы с потоками данных.
- Генераторы, принимающие данные через `send()`, могут вести себя как синхронные корутины, но они отличаются от асинхронных корутин, основанных на `async` и `await`.
